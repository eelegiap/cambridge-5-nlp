{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and load English language model\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in bytes_data.txt**\n",
    "\n",
    "bytes_data.txt is a binary file that contains a list of spacy docs\n",
    "\n",
    "spacy docs are containers for accessing linguistic annotations (https://spacy.io/api/doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of docs 45822\n",
      "example doc: He probably wished to desert but did not wish to give himself up to the allies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in the pre-processed file bytes_data.txt\n",
    "    # this will take a second\n",
    "with open('/Users/paigelee/Desktop/cambridge5/bytes_data.txt', \"rb\") as f:\n",
    "    byte_string = f.read()\n",
    "    \n",
    "new_doc_bin = DocBin().from_bytes(byte_string)\n",
    "# create docs, a list of spacy docs\n",
    "docs = list(new_doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "print('# of docs:', len(docs))\n",
    "print('example doc:', docs[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting entities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the make_entity_dict() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def make_is_person_dict(docs):\n",
    "    \"\"\"makes dictionary where keys are proper nouns\n",
    "    and values are bools of whether or not it is labeled PERSON by Spacy\"\"\"\n",
    "    # intialize entity type dict\n",
    "    is_person_dict = dict()\n",
    "    # initialize all proper nouns to False\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'PROPN':\n",
    "                is_person_dict[token.text] = False   \n",
    "    # set True for entities labeled as PERSON by Spacy\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'PROPN' and token.ent_type_ == 'PERSON':\n",
    "                is_person_dict[token.text] = True\n",
    "    return is_person_dict\n",
    "\n",
    "def valid_to_add(previous_token, token, is_person_dict):\n",
    "    \"\"\"helps determines whether a token is valid to add as an entity (or prefix to entity)\"\"\"\n",
    "    # token is a proper noun and is labeled as a PERSON type entity at least once\n",
    "    if token.pos_ == 'PROPN':\n",
    "        if is_person_dict[token.text]:\n",
    "            return True\n",
    "    # token is of all uppercase form (like ARTIST or ZIGZAG)\n",
    "    if token.text.isupper():\n",
    "        return True\n",
    "    # we count I (not a proper noun) as an entity because we want \n",
    "        # the connections between the author and the other people\n",
    "    if token.text == 'I':\n",
    "        return True\n",
    "    # if the previous token is a person's title, the current token should be an entity\n",
    "    if previous_token.text in ['Mr.', 'Mr', 'Mrs.', 'Sir']:\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def normalize_name(PERSON, entity_dict):\n",
    "    \"\"\"normalizes entity name\"\"\"\n",
    "    # corrects D.G to D.G.\n",
    "    if PERSON + '.' in entity_dict.keys():\n",
    "        return PERSON + '.'\n",
    "    # corrects A to A. (if A. is not already in the dictionary)    \n",
    "    elif len(PERSON) == 1 and PERSON.isalpha():\n",
    "        return PERSON + '.'\n",
    "    # corrects C.S.S . to C.S.S.\n",
    "    elif PERSON.endswith(' .'):\n",
    "        return PERSON.replace(' .', '.')\n",
    "    # corrects . U.S. to U.S.\n",
    "    elif PERSON.startswith('. '):\n",
    "        return PERSON.replace('. ', '')\n",
    "    # else no change to entity name\n",
    "    else:\n",
    "        return PERSON\n",
    "    \n",
    "def invalid_name(PERSON):\n",
    "    \"\"\"gets rid of invalid type names\"\"\"\n",
    "    # invalid name if starts with lowercase letter\n",
    "    if PERSON[0].islower():\n",
    "        return True\n",
    "    # if the 'entity' captured is actually just a punctuation mark or space\n",
    "    if PERSON in ['.', ' ','-']:\n",
    "        return True\n",
    "    # if the 'entity' cpatured is just a trailing \"'s\"\n",
    "    if PERSON.lower() == \"'s\":\n",
    "        return True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making the entity dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an entity dictionary where the keys are entities like Kim Philby, CHRUCHILL, D.G. \n",
    "\n",
    "and the values are lists of sentences (actually Spacy docs) that include the entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entity_dict(docs, MIN_NUM):\n",
    "    \"\"\"makes a dictionary with entities as keys and \n",
    "    docs as values (that include the given entity)\"\"\"\n",
    "    \n",
    "    # initialize entity type dict\n",
    "    is_person_dict = make_is_person_dict(docs)\n",
    "    # initialize the entity dict (output dict)\n",
    "    entity_dict = dict()\n",
    "    \n",
    "    for doc in docs:\n",
    "        # sequences_found_in_sentence is a list of proper noun sequences found in a doc\n",
    "            # [[(text, pos, ent_type),(),()], propn_seq, propn_seq, ...]\n",
    "        sequences_found_in_sentence = []\n",
    "        # prop_seq captures each sequence of proper nouns\n",
    "            # for example, [('Ted', 'PROPN', 'PERSON'),('Johnson', 'PROPN', 'PERSON')]\n",
    "            # often, len(prop_seq) == 1 because many proper nouns are just one token long\n",
    "        propn_seq = []\n",
    "        # set placeholder value for first iteration of previous_token per doc\n",
    "        previous_token = nlp('')\n",
    "        # loop over tokens in doc and check if they are valid to add to a proper noun sequence\n",
    "        for token in doc:\n",
    "            if valid_to_add(previous_token, token, is_person_dict):\n",
    "                propn_seq.append((token.text, token.ent_type_))\n",
    "            else:\n",
    "                # once you hit a token that is NOT a proper noun, \n",
    "                    # add cumulative propn_seq to sequences_found_in_sentence\n",
    "                if propn_seq != []:\n",
    "                    sequences_found_in_sentence.append(propn_seq)\n",
    "                    propn_seq = []\n",
    "            # set token to use in next valid_to_add()\n",
    "            previous_token = token\n",
    "\n",
    "        people = []\n",
    "        # create people, a list of names based on proper noun sequences in the sentence\n",
    "        for propn_seq in sequences_found_in_sentence:\n",
    "            # if the current proper noun sequence includes more than one token\n",
    "                # construct a compound proper noun name\n",
    "            if len(propn_seq) > 1:\n",
    "                names = []\n",
    "                for name, ent in propn_seq:\n",
    "                    names.append(name)\n",
    "                people.append(' '.join(names))\n",
    "            # if there is only one proper noun in the sequence, add that \n",
    "            else:\n",
    "                people.append(propn_seq[0][0])\n",
    "\n",
    "        # add normalized name to dictionary\n",
    "        for PERSON in people:\n",
    "            if invalid_name(PERSON):\n",
    "                break\n",
    "            # normalize the name based on spelling rules\n",
    "            PERSON = normalize_name(PERSON, entity_dict)\n",
    "            # add name and doc to dictionary                \n",
    "            if PERSON not in entity_dict.keys():\n",
    "                entity_dict[PERSON] = [doc]\n",
    "            else:\n",
    "                entity_dict[PERSON].append(doc)\n",
    "                \n",
    "    # check if the PERSON occurs MIN_NUM or more times\n",
    "    not_enough_occurrences = []\n",
    "    for PERSON in entity_dict.keys():\n",
    "        # create list of entities with too few occurrences\n",
    "        if len(entity_dict[PERSON]) < MIN_NUM:\n",
    "            not_enough_occurrences.append(PERSON)\n",
    "    # delete entity from dictionary if it doesn't occur enough times\n",
    "    for PERSON in not_enough_occurrences:\n",
    "        del entity_dict[PERSON]\n",
    "    print('entity dict succesfully created.\\n')\n",
    "    print('# of proper nouns found:', len(entity_dict))\n",
    "    print('\\nusage example:\\n\\tentity_dict[\"Philby\"][0] =', entity_dict['Philby'][0])\n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity dict succesfully created.\n",
      "\n",
      "# of proper nouns found: 1476\n",
      "\n",
      "usage example:\n",
      "\tentity_dict[\"Philby\"][0] = We have received a letter from Philby showing the action of the Portuguese authorities on the representations made by us about German in espionage in Portugal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# will probably take 15-30 seconds to run\n",
    "entity_dict = make_entity_dict(docs, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a list of entities sorted in alphabetical order**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the first 20, but can be accessed by entities_sorted_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.C. 8\n",
      "0.S.S. 5\n",
      "1.B. 5\n",
      "18B. 24\n",
      "A Jap B.J. 5\n",
      "A. 625\n",
      "A. A. 12\n",
      "A.A. 30\n",
      "A.B. 7\n",
      "A.C.E. 9\n",
      "A.D.N.I. 11\n",
      "A.G. 15\n",
      "A.G.3 6\n",
      "A.L.O. 11\n",
      "A.L.O.s 6\n",
      "A.M. 24\n",
      "A.W.S. 5\n",
      "ADNI 11\n",
      "ALCAZAR 9\n",
      "ALEXANDER 7\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for ent in entity_dict.keys():\n",
    "    lst.append((ent, len(entity_dict[ent]), entity_dict[ent]))\n",
    "entities_sorted_by_name = sorted(lst)\n",
    "for PERSON, NUM_ENTRIES, LST in entities_sorted_by_name[:20]:\n",
    "    print(PERSON, NUM_ENTRIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating list of entities sorted in descending count order**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the first 20, but can be accessed by entities_sorted_by_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7946 I.\n",
      "1222 Germans\n",
      "765 D.G.\n",
      "625 A.\n",
      "624 Germany\n",
      "545 C.\n",
      "510 Dick\n",
      "411 SIS\n",
      "377 H.O.\n",
      "363 S.I.S.\n",
      "352 London\n",
      "324 Lisbon\n",
      "301 Eire\n",
      "286 France\n",
      "281 French\n",
      "280 Air\n",
      "271 Abwehr\n",
      "246 Army\n",
      "239 W.\n",
      "239 Committee\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for ent in entity_dict.keys():\n",
    "    lst.append((len(entity_dict[ent]), ent, entity_dict[ent]))\n",
    "entities_sorted_by_count = sorted(lst, reverse = True)\n",
    "for NUM_ENTRIES, PERSON, LST in entities_sorted_by_count[:20]:\n",
    "    print(NUM_ENTRIES, PERSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for members of spy ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cambridge_5 = ['Kim Philby', '']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
